{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d86248e",
   "metadata": {},
   "source": [
    "# W&B Enterprise Workshop: Advanced Geological AI\n",
    "\n",
    "Welcome to the W&B Enterprise Workshop. In this session, we will demonstrate how Weights & Biases serves as the indispensable **system of record** for a complex, enterprise-grade machine learning workflow. By establishing a centralized hub for all our activities, we can break down silos between geoscientists, ML engineers, and stakeholders, creating a single source of truth for the entire project lifecycle.\n",
    "\n",
    "We will simulate the training of a **conditional diffusion model** for geological structure generation. This allows us to focus on the MLOps challenges—collaboration, monitoring, governance, and reporting—that W&B is designed to solve, without waiting hours for a real model to train.\n",
    "\n",
    "Our first step is to install the required libraries and import our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cef1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install minimal dependencies for W&B workshop\n",
    "# The wandb-workspaces library is key for both programmatic workspaces and reporting\n",
    "# allowing us to automatically generate stakeholder-ready W&B Reports later\n",
    "%pip install wandb numpy tqdm wandb-workspaces plotly pillow scikit-image pyvista ipyvolume ipython-genutils trame trame-vuetify trame-vtk -q\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import wandb_workspaces.reports.v2 as wr\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from plotly.subplots import make_subplots\n",
    "import pyvista as pv\n",
    "import ipyvolume as ipv\n",
    "from skimage.transform import resize\n",
    "from textwrap import dedent\n",
    "\n",
    "# Set PyVista to use an off-screen plotter for notebook environments\n",
    "pv.set_jupyter_backend(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417631e",
   "metadata": {},
   "source": [
    "## 1. W&B Project Configuration\n",
    "\n",
    "Here, we'll log in to W&B and define the `ENTITY` (your team or organization) and the `PROJECT` for this workshop. \n",
    "\n",
    "A W&B Project is a collaborative workspace where your entire team—from geoscientists to ML engineers—can track experiments, compare results, and share insights in real-time.\n",
    "\n",
    "Centralizing our work here is the first step toward building a reliable system of record. We also define a sample configuration dictionary that holds our model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2215955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Authentication: Entity = wandb_emea\n",
      "Project: workshop-scratchpad4\n",
      "Training configuration loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ENTITY = \"wandb_emea\"  # Replace with your W&B team entity\n",
    "PROJECT = \"workshop-scratchpad4\"\n",
    "\n",
    "# Ensure you have logged in and defined ENTITY and PROJECT in a previous cell\n",
    "assert \"ENTITY\" in locals(), \"Please define the ENTITY variable\"\n",
    "assert \"PROJECT\" in locals(), \"Please define the PROJECT variable\"\n",
    "\n",
    "# W&B Team Authentication\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "print(f\"W&B Authentication: Entity = {ENTITY}\")\n",
    "print(f\"Project: {PROJECT}\")\n",
    "\n",
    "# Configuration for (simulated) conditional diffusion training\n",
    "# wandb.config makes hyperparameters a first-class citizen.\n",
    "# They are saved with every run, ensuring 100% reproducibility and\n",
    "# enabling powerful, automated hyperparameter sweeps\n",
    "\n",
    "example_config = {\n",
    "    # Model Architecture\n",
    "    \"model_architecture\": \"ConditionalUNet3D\",\n",
    "    \"task\": \"geological_structure_generation\",\n",
    "    \"input_modality\": \"seismic_amplitude\",\n",
    "    \"output_modality\": \"karst_structures\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"epochs\": 10,\n",
    "    \"batches_per_epoch\": 20,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 4,\n",
    "    \n",
    "    # Diffusion Parameters\n",
    "    \"timesteps\": 1000,\n",
    "    \"noise_schedule\": \"cosine\",\n",
    "    \"conditioning_strength\": 0.8,\n",
    "    \n",
    "    # Geological Domain\n",
    "    \"voxel_resolution\": \"25m\",\n",
    "    \"depth_range\": \"0-800m\",\n",
    "    \"geological_context\": \"karst_detection\"\n",
    "}\n",
    "\n",
    "print(\"Training configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4deb684",
   "metadata": {},
   "source": [
    "## 2. Simulation and Visualization Helpers\n",
    "\n",
    "This cell contains the helper functions for our workshop. **This is not part of the core W&B integration.**\n",
    "\n",
    "- `simulate_...`: A function that mimics the behavior of a real conditional diffusion model, generating progressively better geological predictions with each epoch.\n",
    "- `plot_...` & `normalize_...`: Utilities for creating interactive charts and preparing images for visualization.\n",
    "\n",
    "We're using a simulation because these generative models can run for days or weeks. By focusing on the MLOps workflow, we demonstrate how to solve the operational challenges—like monitoring, debugging, and reporting—where teams lose the most time and money, especially when long-running jobs fail silently.\n",
    "\n",
    "By isolating this simulation logic, we can focus the rest of the notebook purely on the MLOps workflow powered by W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d86083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By returning a wandb.Html object, we can log custom, interactive visualizations\n",
    "# like this Plotly chart directly into the W&B dashboard. This allows domain experts\n",
    "# to analyze results without switching contexts or downloading files.\n",
    "def plot_well_log_comparison(gt_log: np.ndarray, pred_log: np.ndarray, depth: np.ndarray) -> wandb.Html:\n",
    "    \"\"\"Creates an interactive Plotly chart comparing ground truth and predicted well logs.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=gt_log, y=depth, mode='lines', name='Ground Truth Log', line=dict(color='black')))\n",
    "    fig.add_trace(go.Scatter(x=pred_log, y=depth, mode='lines', name='Predicted Log', line=dict(color='crimson', dash='dash')))\n",
    "    fig.update_layout(\n",
    "        title=\"Well Log Comparison\",\n",
    "        xaxis_title=\"Signal Amplitude\",\n",
    "        yaxis_title=\"Depth (m)\",\n",
    "        yaxis_autorange='reversed' # Depth increases downwards\n",
    "    )\n",
    "    html = pio.to_html(fig)\n",
    "    return wandb.Html(html)\n",
    "\n",
    "def generate_synthetic_loss(\n",
    "    step: int,\n",
    "    schedule: str = \"cosine\",\n",
    "    conditioning_strength: float = 0.8,\n",
    "    learning_rate: float = 1e-3,\n",
    "    batch_size: int = 4,\n",
    "    base_seed: int = 42,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Parameterized synthetic loss that responds to sweep knobs.\n",
    "    - schedule: shapes the decay (cosine|linear|sigmoid)\n",
    "    - conditioning_strength: higher => faster improvement\n",
    "    - learning_rate: modestly speeds decay\n",
    "    - batch_size: larger => lower noise\n",
    "    Deterministic per step via base_seed + step.\n",
    "    \"\"\"\n",
    "    # Progress in [0, 1]\n",
    "    progress = np.clip(step / 600.0, 0.0, 1.0)\n",
    "\n",
    "    if schedule == \"linear\":\n",
    "        sched = progress\n",
    "    elif schedule == \"sigmoid\":\n",
    "        sched = 1.0 / (1.0 + np.exp(-10.0 * (progress - 0.5)))\n",
    "    else:  # cosine\n",
    "        sched = 0.5 - 0.5 * np.cos(np.pi * progress)\n",
    "\n",
    "    # Faster decay with stronger conditioning and slightly higher LR\n",
    "    decay_speed = 1.0 + 0.8 * conditioning_strength + 0.3 * np.log10(max(learning_rate, 1e-6) / 1e-3 + 1.0)\n",
    "    base_curve = 1.5 * np.exp(-3.0 * sched * decay_speed) + 0.05\n",
    "\n",
    "    # Noise shrinks with batch size\n",
    "    rng = np.random.RandomState(int(base_seed) + int(step))\n",
    "    noise = rng.normal(0.0, 0.05 / max(batch_size, 1))\n",
    "\n",
    "    return float(max(0.005, base_curve + noise))\n",
    "\n",
    "# Simulation controls: \n",
    "# conditioning_strength controls structure emergence\n",
    "# noise_schedule (‘cosine’, ‘linear’, ‘sigmoid’) controls noise decay shape\n",
    "# Runs are deterministic per epoch via base_seed\n",
    "def simulate_conditional_diffusion_progress(\n",
    "    seismic_condition: np.ndarray,\n",
    "    karst_target: np.ndarray,\n",
    "    epoch: int,\n",
    "    total_epochs: int = 10,\n",
    "    conditioning_strength: float = 0.8,\n",
    "    noise_schedule: str = \"cosine\",\n",
    "    base_seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate conditional diffusion learning progression (single-channel).\n",
    "    - conditioning_strength: scales structure emergence (higher = stronger alignment to karst_target)\n",
    "    - noise_schedule: 'cosine' | 'linear' | 'sigmoid' controls noise decay shape over epochs\n",
    "    - Deterministic per epoch via base_seed + epoch\n",
    "    \"\"\"\n",
    "    # Training progress (0.0 at start, 1.0 at end) → used to shape schedule\n",
    "    progress = epoch / float(max(1, total_epochs - 1))\n",
    "\n",
    "    # Deterministic seed derived from base_seed and epoch\n",
    "    np.random.seed(int(base_seed) + int(epoch) * 1000)\n",
    "\n",
    "    # Schedule shaping (affects how fast noise decreases / structure increases)\n",
    "    if noise_schedule == \"linear\":\n",
    "        schedule_factor = progress\n",
    "    elif noise_schedule == \"sigmoid\":\n",
    "        schedule_factor = 1.0 / (1.0 + np.exp(-12.0 * (progress - 0.5)))\n",
    "    else:  # \"cosine\" default\n",
    "        schedule_factor = 0.5 - 0.5 * np.cos(np.pi * progress)\n",
    "\n",
    "    # Stage 1 (epochs 0–3): learn basic correlations; high noise\n",
    "    if epoch <= 3:\n",
    "        noise_level = 0.8 - (epoch / 3.0) * 0.4          # 0.8 → 0.4\n",
    "        noise_level *= (1.0 - 0.25 * schedule_factor)    # slightly faster decay with schedule\n",
    "        structural_learning = (epoch / 3.0) * 0.3        # 0.0 → 0.3\n",
    "        structural_learning *= conditioning_strength\n",
    "\n",
    "        structure_mask = (seismic_condition > np.percentile(seismic_condition, 60)).astype(float)\n",
    "        noise = np.random.normal(0, noise_level, seismic_condition.shape)\n",
    "\n",
    "        prediction = (\n",
    "            noise * 0.7 +\n",
    "            seismic_condition * structural_learning +\n",
    "            karst_target * structure_mask * 0.1\n",
    "        )\n",
    "\n",
    "    # Stage 2 (epochs 4–6): refine geological structures; reduce noise\n",
    "    elif epoch <= 6:\n",
    "        stage_progress = (epoch - 4.0) / 2.0             # 0.0 → 1.0\n",
    "        karst_regions = (\n",
    "            (seismic_condition > np.percentile(seismic_condition, 40)) &\n",
    "            (seismic_condition < np.percentile(seismic_condition, 85))\n",
    "        )\n",
    "\n",
    "        noise_level = 0.4 - stage_progress * 0.25        # 0.4 → 0.15\n",
    "        noise_level *= (1.0 - 0.25 * schedule_factor)\n",
    "        noise = np.random.normal(0, noise_level, seismic_condition.shape)\n",
    "\n",
    "        geological_understanding = 0.3 + stage_progress * 0.4   # 0.3 → 0.7\n",
    "        geological_understanding *= conditioning_strength\n",
    "\n",
    "        prediction = (\n",
    "            noise * 0.3 +\n",
    "            karst_target * karst_regions * geological_understanding +\n",
    "            seismic_condition * 0.2 * (1 - karst_regions)\n",
    "        )\n",
    "\n",
    "    # Stage 3 (epochs 7–9): fine-tuning; high accuracy, low noise\n",
    "    else:\n",
    "        stage_progress = (epoch - 7.0) / 2.0             # 0.0 → 1.0\n",
    "        noise_level = 0.15 - stage_progress * 0.10       # 0.15 → 0.05\n",
    "        noise_level *= (1.0 - 0.25 * schedule_factor)\n",
    "        noise = np.random.normal(0, noise_level, seismic_condition.shape)\n",
    "\n",
    "        accuracy = 0.7 + stage_progress * 0.25           # 0.7 → 0.95\n",
    "        accuracy *= (0.85 + 0.3 * conditioning_strength)\n",
    "        accuracy = np.clip(accuracy, 0.0, 1.0)\n",
    "\n",
    "        uncertainty_mask = np.random.random(seismic_condition.shape) < 0.1\n",
    "\n",
    "        prediction = (\n",
    "            karst_target * accuracy +\n",
    "            noise * 0.1 +\n",
    "            seismic_condition * uncertainty_mask * 0.05\n",
    "        )\n",
    "\n",
    "    # Keep realistic value ranges and avoid exact zeros in residuals\n",
    "    prediction = np.clip(prediction, 0, 1)\n",
    "    prediction += np.random.uniform(-1e-9, 1e-9, prediction.shape)\n",
    "    return prediction\n",
    "\n",
    "def normalize_for_visualization(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize geological data for W&B visualization with proper contrast.\"\"\"\n",
    "    data_min, data_max = np.min(data), np.max(data)\n",
    "    if data_max == data_min:\n",
    "        return np.full_like(data, 0.5)\n",
    "    normalized = (data - data_min) / (data_max - data_min)\n",
    "    # Apply gamma correction for better geological feature visibility\n",
    "    return np.power(normalized, 0.7)\n",
    "\n",
    "def simulate_forward_model(x_pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulates a forward model y=f(x) to generate a predicted condition y_pred.\n",
    "    This step is crucial for evaluating the \"cycle consistency\" of our generator;\n",
    "    we check if the forward model of our prediction (y_pred) matches the original input condition (y).\n",
    "    In a real scenario, this would be a physics-based or learned model.\n",
    "    Here, we simulate it by applying a slight blur and adding minor noise.\n",
    "    \"\"\"\n",
    "    # Apply a simple blurring effect (convolution with a small kernel)\n",
    "    kernel = np.ones((3, 3, 3)) / 27.0\n",
    "    # Use scipy for convolution if available, otherwise a simpler method\n",
    "    try:\n",
    "        from scipy.ndimage import convolve\n",
    "        blurred = convolve(x_pred, kernel, mode='reflect')\n",
    "    except ImportError:\n",
    "        # Fallback if scipy is not installed\n",
    "        blurred = x_pred\n",
    "        \n",
    "    # Add a small amount of random noise\n",
    "    noise = np.random.normal(0, 0.02, x_pred.shape)\n",
    "    y_pred = blurred + noise\n",
    "    \n",
    "    return np.clip(y_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077d07b",
   "metadata": {},
   "source": [
    "### Tier 1 Analysis: Global Sanity Check (Downsampled View)\n",
    "\n",
    "**The Question:** \"Is the model globally stable?\" Before wasting time on details, we need a quick, high-level check of the entire volume.\n",
    "\n",
    "**The Technique:** We downsample the high-resolution volumes (e.g., 128³) to a smaller size (64³) and render them side-by-side. This provides an immediate, low-latency overview.\n",
    "\n",
    "**What We Look For:**\n",
    "* **Large-Scale Artifacts:** Are there obvious blocky patterns or spatial biases where the model is systematically failing?\n",
    "* **Structural Coherence:** Does the overall shape of the prediction roughly match the ground truth?\n",
    "* **Performance:** This view renders instantly in the browser, providing the fastest possible feedback loop during training.\n",
    "\n",
    "**The Limitation:**\n",
    "* **Loss of Detail:** This view is for macro-level assessment only. Fine-grained geological textures and sharp boundaries are intentionally sacrificed for speed and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b0aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_downsampled_comparison(volumes: Dict[str, np.ndarray]) -> go.Figure:\n",
    "    \"\"\"Downsamples volumes and returns a side-by-side Plotly figure.\"\"\"\n",
    "    target_shape = (64, 64, 64)\n",
    "    subplot_titles = [name.replace(\"_\", \" \") for name in volumes.keys()]\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=len(volumes),\n",
    "        specs=[[{'type': 'volume'}] * len(volumes)],\n",
    "        subplot_titles=subplot_titles\n",
    "    )\n",
    "    for i, (name, data) in enumerate(volumes.items()):\n",
    "        resized_data = resize(data, target_shape, anti_aliasing=True)\n",
    "        vmin, vmax = float(np.min(resized_data)), float(np.max(resized_data))\n",
    "        if vmax == vmin: vmax = vmin + 1.0\n",
    "        fig.add_trace(\n",
    "            go.Volume(\n",
    "                value=resized_data,                 # pass 3D array directly\n",
    "                cmin=vmin, cmax=vmax,\n",
    "                isomin=vmin + (vmax - vmin) * 0.02,\n",
    "                isomax=vmax - (vmax - vmin) * 0.02,\n",
    "                opacity=0.15, opacityscale=\"uniform\",\n",
    "                surface_count=12,\n",
    "                colorscale='RdBu' if ('Residual' in name or 'Error' in name) else 'viridis',\n",
    "            ),\n",
    "            row=1, col=i + 1\n",
    "        )\n",
    "    fig.update_layout(title_text=\"Downsampled 3D Comparison\", height=420, margin=dict(t=50, b=10, l=10, r=10))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77f75e",
   "metadata": {},
   "source": [
    "### Tier 2 Analysis: Fine-Detail Inspection (Cropped View)\n",
    "\n",
    "**The Question:** \"Now that the global structure looks right, is the model accurately preserving fine details?\"\n",
    "\n",
    "**The Technique:** We extract a smaller cube (e.g., 64³) from the center of the original, full-resolution volumes. This allows for a direct, apples-to-apples comparison of a specific region without any downsampling.\n",
    "\n",
    "**What We Look For:**\n",
    "* **Texture and Sharpness:** Is the model generating geologically plausible textures, or is the output blurry and overly smooth? Are the boundaries between different rock types sharp and well-defined?\n",
    "* **Small-Scale Accuracy:** This is crucial for validating that the model isn't just learning a low-resolution approximation but is capturing the high-frequency details essential for interpretation.\n",
    "\n",
    "**The Limitation:**\n",
    "* **Loss of Context:** This view provides no information about performance outside the cropped region. An issue might seem small here but could be part of a much larger problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8406a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cropped_comparison(volumes: Dict[str, np.ndarray]) -> go.Figure:\n",
    "    \"\"\"Crops the center of volumes and returns a side-by-side Plotly figure.\"\"\"\n",
    "    crop_size = 64\n",
    "    subplot_titles = [name.replace(\"_\", \" \") for name in volumes.keys()]\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=len(volumes),\n",
    "        specs=[[{'type': 'volume'}] * len(volumes)],\n",
    "        subplot_titles=subplot_titles\n",
    "    )\n",
    "    for i, (name, data) in enumerate(volumes.items()):\n",
    "        if all(dim >= crop_size for dim in data.shape):\n",
    "            center = [dim // 2 for dim in data.shape]\n",
    "            start = [c - crop_size // 2 for c in center]\n",
    "            cropped_data = data[\n",
    "                start[0]:start[0]+crop_size,\n",
    "                start[1]:start[1]+crop_size,\n",
    "                start[2]:start[2]+crop_size\n",
    "            ]\n",
    "        else:\n",
    "            cropped_data = data\n",
    "        vmin, vmax = float(np.min(cropped_data)), float(np.max(cropped_data))\n",
    "        if vmax == vmin: vmax = vmin + 1.0\n",
    "        fig.add_trace(\n",
    "            go.Volume(\n",
    "                value=cropped_data,                # pass 3D array directly\n",
    "                cmin=vmin, cmax=vmax,\n",
    "                isomin=vmin + (vmax - vmin) * 0.02,\n",
    "                isomax=vmax - (vmax - vmin) * 0.02,\n",
    "                opacity=0.15, opacityscale=\"uniform\",\n",
    "                surface_count=12,\n",
    "                colorscale='RdBu' if ('Residual' in name or 'Error' in name) else 'viridis',\n",
    "            ),\n",
    "            row=1, col=i + 1\n",
    "        )\n",
    "    fig.update_layout(title_text=\"Cropped (Full-Res) 3D Comparison\", height=420, margin=dict(t=50, b=10, l=10, r=10))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06aea2",
   "metadata": {},
   "source": [
    "### Tier 3 Analysis: Debugging with the Residual (High-Fidelity Render)\n",
    "\n",
    "**The Question:** \"Where, specifically, is my model failing?\" To improve the model, we must understand the nature and location of its errors.\n",
    "\n",
    "**The Technique:** We use a high-performance library like Ipyvolume or PyVista to create the best possible render of the most critical volume: **the residual** (`AI_Condition_Y_pred - Input_Condition_Y`). This shows us the error map in 3D.\n",
    "\n",
    "**What We Look For:**\n",
    "* **Systematic Errors:** Are the largest errors concentrated in a specific geological layer or region? This could indicate a need for more training data from that area.\n",
    "* **Nature of the Error:** Is the model over-predicting (large positive residual) or under-predicting (large negative residual)? Advanced rendering controls (lighting, opacity) allow us to peel back the volume layer-by-layer to diagnose the problem. This is the view a domain expert would use to provide targeted feedback.\n",
    "\n",
    "**The Trade-off:**\n",
    "* **Single-Volume Focus:** This is a deep-dive tool, best used to inspect one critical volume at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7868f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pyvista_render(volume: np.ndarray, title: str) -> str:\n",
    "    \"\"\"\n",
    "    Render a single volume with PyVista and return HTML.\n",
    "    - Downsamples large volumes to speed export\n",
    "    - Uses ImageData; compatible with older PyVista versions\n",
    "    \"\"\"\n",
    "    html_filename = f\"{title.replace(' ', '_').replace('(', '').replace(')', '')}_temp.html\"\n",
    "    try:\n",
    "        # Downsample aggressively if needed (keep <= 96^3)\n",
    "        max_dim = 96\n",
    "        dz, dy, dx = map(int, volume.shape)\n",
    "        scale = max(1, max(dz, dy, dx) // max_dim)\n",
    "        target = (max(1, dz // scale), max(1, dy // scale), max(1, dx // scale))\n",
    "        vol_ds = volume if (dz, dy, dx) == target else resize(volume, target, anti_aliasing=True)\n",
    "        vol_ds = np.asarray(vol_ds, dtype=np.float32, order=\"F\")\n",
    "\n",
    "        # Build an ImageData grid (works across PyVista versions)\n",
    "        nz, ny, nx = vol_ds.shape  # z, y, x\n",
    "        grid = pv.ImageData(dimensions=(nx, ny, nz))  # note order: x, y, z\n",
    "        grid.spacing = (1.0, 1.0, 1.0)\n",
    "        grid.origin = (0.0, 0.0, 0.0)\n",
    "        grid.point_data[\"values\"] = vol_ds.flatten(order=\"F\")\n",
    "\n",
    "        plotter = pv.Plotter(off_screen=True, notebook=True, window_size=(800, 600))\n",
    "        plotter.add_volume(\n",
    "            grid,\n",
    "            scalars=\"values\",\n",
    "            cmap=\"RdBu\" if (\"Residual\" in title or \"Error\" in title) else \"viridis\",\n",
    "            opacity=\"sigmoid\",\n",
    "            shade=True,\n",
    "        )\n",
    "        plotter.add_axes()\n",
    "        plotter.export_html(html_filename)\n",
    "        html = Path(html_filename).read_text()\n",
    "        plotter.close()\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        return f\"<p>PyVista rendering failed: {e}</p>\"\n",
    "    finally:\n",
    "        try:\n",
    "            if os.path.exists(html_filename):\n",
    "                os.remove(html_filename)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2ba0e",
   "metadata": {},
   "source": [
    "## 3. The Core Training Function with W&B Integration\n",
    "\n",
    "This function encapsulates the entire training and evaluation workflow. It serves as a blueprint for a reproducible, auditable, and transparent ML lifecycle. Each section is clearly marked to show how a specific W&B feature is used to track, version, and evaluate the model, turning a standard training script into an enterprise-ready system of record.\n",
    "\n",
    "**Notation and channel scope**  \n",
    "X: ground-truth geology (karst), X_pred: generated prediction  \n",
    "Y: input seismic condition, Y_pred = f(X_pred)  \n",
    "Residual = Y_pred − Y  \n",
    "\n",
    "Single-channel visuals: This demo renders a single channel for stability and speed. Multi-channel rendering/metrics are feasible (loop over channels and log per-channel), but intentionally out of scope here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40728477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-61' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    }
   ],
   "source": [
    "def train_conditional_diffusion(config=None):\n",
    "    \"\"\"\n",
    "    Train a conditional diffusion model for geological structure generation.\n",
    "    This function is instrumented with W&B to track the entire lifecycle.\n",
    "    \"\"\"\n",
    "    # ===================================================================\n",
    "    # 1. W&B SETUP: Initialize Run and Centralize Configuration\n",
    "    #    - wandb.init() starts a new run to track this experiment.\n",
    "    #    - wandb.config pulls hyperparameter settings, making them available to sweeps.\n",
    "    # ===================================================================\n",
    " \n",
    "\n",
    "    # Initialize a new W&B run\n",
    "    run = wandb.init(\n",
    "        entity=ENTITY,\n",
    "        project=PROJECT,\n",
    "        config=(config or example_config), # use config if it exists, otherwise use example_config (useful for sweeps)\n",
    "        # job_type and tags are powerful organizational tools that make it easy\n",
    "        # to filter, group, and compare runs across a large project in the W&B UI\n",
    "        job_type=\"training\",\n",
    "        tags=[\"conditional-diffusion\"]\n",
    "    )\n",
    "\n",
    "    # Use the config from W&B (this allows sweeps to override values)\n",
    "    config = wandb.config # this is a dictionary of the hyperparameters\n",
    "    np.random.seed(config.get(\"seed\", 42))  # set global seed once\n",
    "    # ===================================================================\n",
    "    # 2. W&B ARTIFACTS: Versioning and Loading the Dataset\n",
    "    #    - run.use_artifact() declares a dependency on a specific dataset version.\n",
    "    #    - This creates a lineage graph, giving us a full audit trail.\n",
    "    # ===================================================================\n",
    "\n",
    "    # ===================================================================\n",
    "    # 2. W&B ARTIFACTS: Versioning and Loading the Dataset\n",
    "    #    - run.use_artifact() declares a dependency on a specific dataset version.\n",
    "    #    - This creates a lineage graph, giving us a full audit trail.\n",
    "    # ===================================================================\n",
    "    # Load CigKarst dataset from W&B Registry\n",
    "    \n",
    "    \n",
    "    # run.use_artifact() creates a direct link to the dataset version used for this run.\n",
    "    # This automatically generates a complete, visual data-to-model lineage graph,\n",
    "    # which is essential for auditability and debugging.\n",
    "    artifact = run.use_artifact('wandb-registry-dataset/CigKarst:v0', type='dataset')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    # Load geological samples from the dataset\n",
    "    geological_samples = []\n",
    "    metadata_path = Path(artifact_dir) / \"dataset_metadata.npz\"\n",
    "\n",
    "    if metadata_path.exists():\n",
    "        metadata = np.load(metadata_path, allow_pickle=True)\n",
    "        samples_data = metadata['samples'].tolist()\n",
    "        for sample_info in samples_data:\n",
    "            seismic_file = Path(artifact_dir) / sample_info['seismic_file']\n",
    "            karst_file = Path(artifact_dir) / sample_info['karst_file']\n",
    "            if seismic_file.exists() and karst_file.exists():\n",
    "                seismic_data = np.load(seismic_file)\n",
    "                karst_data = np.load(karst_file)\n",
    "                sample = {\n",
    "                    'seismic': seismic_data['patch'],\n",
    "                    'karst': karst_data['patch'],\n",
    "                    'sample_id': sample_info['sample_id'],\n",
    "                    'coordinates': sample_info['coordinates'],\n",
    "                    'source': sample_info['source_volume']\n",
    "                }\n",
    "                geological_samples.append(sample)\n",
    "        print(f\"Loaded {len(geological_samples)} real geological samples from CigKarst\")\n",
    "    else:\n",
    "        print(\"Metadata file not found in artifact\")\n",
    "        run.finish()\n",
    "        return None\n",
    "\n",
    "    # Select a fixed set of samples for consistent validation across runs\n",
    "    good_sample_ids = [\"volume_0_patch_1\", \"volume_0_patch_3\", \"volume_1_patch_0\"]\n",
    "    fixed_samples = [s for s in geological_samples if s['sample_id'] in good_sample_ids]\n",
    "    if len(fixed_samples) != len(good_sample_ids):\n",
    "        raise ValueError(f\"Error: Could not find all specified good samples.\")\n",
    "    print(f\"Selected {len(fixed_samples)} pre-validated samples for tracking.\")\n",
    "\n",
    "    # Track best model\n",
    "    best_validation_loss = float('inf')\n",
    "\n",
    "    # ===================================================================\n",
    "    # 3. W&B TRAINING LOOP: Logging Live Metrics\n",
    "    #    - run.log() is called inside the loop to stream metrics in real-time.\n",
    "    #    - This allows us to monitor model performance and system utilization live from the dashboard.\n",
    "    # ===================================================================\n",
    "    for epoch in range(config.get(\"epochs\", 10)):\n",
    "        print(f\"Epoch {epoch}: Simulating conditional diffusion training...\")\n",
    "        batches_per_epoch = config.get(\"batches_per_epoch\", 20)\n",
    "        \n",
    "        for batch in range(batches_per_epoch):\n",
    "            step = epoch * batches_per_epoch + batch\n",
    "            # Simulate training progress and calculate synthetic losses\n",
    "            noise_pred_loss = generate_synthetic_loss(\n",
    "            step,\n",
    "            schedule=str(config.get(\"noise_schedule\", \"cosine\")),\n",
    "            conditioning_strength=float(config.get(\"conditioning_strength\", 0.8)),\n",
    "            learning_rate=float(config.get(\"learning_rate\", 1e-3)),\n",
    "            batch_size=int(config.get(\"batch_size\", 4)),\n",
    "            base_seed=int(config.get(\"seed\", 42)),\n",
    "            )\n",
    "            reconstruction_proxy = generate_synthetic_loss(\n",
    "            int(step * 1.5),\n",
    "            schedule=str(config.get(\"noise_schedule\", \"cosine\")),\n",
    "            conditioning_strength=float(config.get(\"conditioning_strength\", 0.8)),\n",
    "            learning_rate=float(config.get(\"learning_rate\", 1e-3)),\n",
    "            batch_size=int(config.get(\"batch_size\", 4)),\n",
    "            base_seed=int(config.get(\"seed\", 42)),\n",
    "             )\n",
    "            total_loss = 0.6 * noise_pred_loss + 0.4 * reconstruction_proxy\n",
    "\n",
    "            # Prepare log dictionary for training metrics\n",
    "            # Training-time proxy metrics (synthetic): reconstruction_mse approximates X_pred vs X; true condition-consistency is computed at validation.\n",
    "            log_dict = {\n",
    "            \"train/noise_prediction_loss\": noise_pred_loss,\n",
    "            \"train/reconstruction_mse\": reconstruction_proxy,  # X_pred vs X (synthetic proxy during training)\n",
    "            \"train/total_loss\": total_loss,\n",
    "            \"train/learning_rate\": config.get(\"learning_rate\", 1e-3) * (0.95 ** (epoch // 3))\n",
    "            }\n",
    "            \n",
    "            # SINGLE wandb.log call per training step. W&B automatically captures system\n",
    "            # metrics (CPU/GPU utilization, memory) alongside your custom metrics, providing\n",
    "            # a holistic view to diagnose performance bottlenecks in real-time.\n",
    "            run.log(log_dict)\n",
    "\n",
    "        # ===================================================================\n",
    "        # 4. W&B VALIDATION: Logging Rich Media and Tables\n",
    "        #    - At the end of each epoch, we log qualitative results.\n",
    "        #    - wandb.Image() for visual comparison.\n",
    "        #    - wandb.Html() for custom, interactive Plotly charts.\n",
    "        #    - wandb.Table() to create structured, sortable tables of predictions.\n",
    "        # ===================================================================\n",
    "        \n",
    "        \n",
    "        # wandb.Table creates a rich, interactive table in the UI. This allows for\n",
    "        # sorting and filtering results, and comparing images, plots, and metrics\n",
    "        # side-by-side across different runs—all within a single view.\n",
    "        validation_table = wandb.Table(columns=[\n",
    "        \"epoch\", \"sample_id\", \"seismic_input\", \"ground_truth\",\n",
    "        \"prediction\", \"residual_map\", \"well_log_comparison\",\n",
    "        \"reconstruction_mse\", \"condition_consistency_mse\", \"ssim_score\", \"log_correlation\"\n",
    "        ])\n",
    "            \n",
    "        total_reconstruction_mse = 0.0\n",
    "        total_condition_consistency_mse = 0.0\n",
    "\n",
    "        # --- NEW: Create visualizations for a small BATCH of validation samples ---\n",
    "        # We will create a unique key for each visualization in the log dictionary.\n",
    "        visualizations_log = {}\n",
    "        \n",
    "        for sample in fixed_samples:\n",
    "            \n",
    "            # Generate the necessary volumes for this specific sample\n",
    "            prediction_3d = simulate_conditional_diffusion_progress(\n",
    "            sample['seismic'],\n",
    "            sample['karst'],\n",
    "            epoch,\n",
    "            total_epochs=int(config.get(\"epochs\", 10)),\n",
    "            conditioning_strength=float(config.get(\"conditioning_strength\", 0.8)),\n",
    "            noise_schedule=str(config.get(\"noise_schedule\", \"cosine\")),\n",
    "            base_seed=int(config.get(\"seed\", 42)),\n",
    "            )\n",
    "\n",
    "            y_pred_3d = simulate_forward_model(prediction_3d)\n",
    "            volumes_for_viz = {\n",
    "                \"Input_Seismic_(Y)\": sample['seismic'],\n",
    "                \"Ground_Truth_Karst_(X)\": sample['karst'],\n",
    "                \"Predicted_Karst_(X_pred)\": prediction_3d,\n",
    "                \"Forward_Model_Seismic_(Y_pred)\": y_pred_3d,\n",
    "                \"Model_Error_(Residual)\": y_pred_3d - sample['seismic']\n",
    "            }\n",
    "            \n",
    "            #start\n",
    "            # 3D: zero/mid/last only; single-channel; PyVista default; ipyvolume off by default.\n",
    "            epochs_total = int(config.get(\"epochs\", 10))\n",
    "            mid_epoch = (epochs_total - 1) // 2\n",
    "            should_log_3d = bool(config.get(\"enable_3d\", True)) and (epoch in [0, mid_epoch, epochs_total - 1])\n",
    "\n",
    "            if should_log_3d and sample['sample_id'] == fixed_samples[0]['sample_id']:\n",
    "                volumes_for_viz = {\n",
    "                    \"Predicted_Karst_(X_pred)\": prediction_3d,\n",
    "                    \"Forward_Model_Seismic_(Y_pred)\": y_pred_3d,\n",
    "                    \"Model_Error_(Residual)\": y_pred_3d - sample['seismic'],\n",
    "                }\n",
    "                ''' Removing the plotly figures for now\n",
    "                ds_fig = create_downsampled_comparison({\n",
    "                    \"Predicted_Karst_(X_pred)\": volumes_for_viz[\"Predicted_Karst_(X_pred)\"],\n",
    "                    \"Forward_Model_Seismic_(Y_pred)\": volumes_for_viz[\"Forward_Model_Seismic_(Y_pred)\"],\n",
    "                    \"Model_Error_(Residual)\": volumes_for_viz[\"Model_Error_(Residual)\"],\n",
    "                })\n",
    "                visualizations_log[\"3D/Tier1_Downsampled\"] = wandb.Plotly(ds_fig)\n",
    "\n",
    "                cr_fig = create_cropped_comparison({\n",
    "                    \"Predicted_Karst_(X_pred)\": volumes_for_viz[\"Predicted_Karst_(X_pred)\"],\n",
    "                    \"Forward_Model_Seismic_(Y_pred)\": volumes_for_viz[\"Forward_Model_Seismic_(Y_pred)\"],\n",
    "                    \"Model_Error_(Residual)\": volumes_for_viz[\"Model_Error_(Residual)\"],\n",
    "                })\n",
    "                visualizations_log[\"3D/Tier2_Cropped\"] = wandb.Plotly(cr_fig)\n",
    "                '''\n",
    "                if bool(config.get(\"enable_high_fidelity_3d\", True)):\n",
    "                    visualizations_log[\"Viz/PyVista_Renders/AI_Prediction\"] = wandb.Html(\n",
    "                        create_pyvista_render(volumes_for_viz[\"Predicted_Karst_(X_pred)\"], \"PV_Prediction\")\n",
    "                    )\n",
    "                    visualizations_log[\"Viz/PyVista_Renders/Model_Error\"] = wandb.Html(\n",
    "                        create_pyvista_render(volumes_for_viz[\"Model_Error_(Residual)\"], \"PV_Error\")\n",
    "                    )\n",
    "\n",
    "                if bool(config.get(\"enable_ipyvolume\", False)):\n",
    "                    visualizations_log[\"Viz/ipyvolume_Renders/AI_Prediction\"] = wandb.Html(\n",
    "                        create_ipyvolume_render(volumes_for_viz[\"Predicted_Karst_(X_pred)\"], \"IPV_Prediction\")\n",
    "                    )\n",
    "                    visualizations_log[\"Viz/ipyvolume_Renders/Model_Error\"] = wandb.Html(\n",
    "                        create_ipyvolume_render(volumes_for_viz[\"Model_Error_(Residual)\"], \"IPV_Error\")\n",
    "                    )\n",
    "\n",
    "            # Prepare data for logging (2D slices, logs, etc.)\n",
    "            # Single-channel rendering: we use the scalar volume (or channel 0) for slices.\n",
    "            # Multi-channel is feasible later by indexing and looping channels.\n",
    "            slice_idx = sample['seismic'].shape[2] // 2\n",
    "            seismic_slice = sample['seismic'][:, :, slice_idx]\n",
    "            gt_slice = sample['karst'][:, :, slice_idx]\n",
    "            pred_slice = prediction_3d[:, :, slice_idx]\n",
    "            residual_slice = pred_slice - gt_slice\n",
    "            max_abs_val = np.max(np.abs(residual_slice))\n",
    "            residual_norm = (residual_slice + max_abs_val) / (2 * max_abs_val) if max_abs_val > 0 else np.zeros_like(residual_slice)\n",
    "\n",
    "            # Per-epoch 2D diagnostic: compact 2x2 central-slice grid for one representative sample (single-channel).\n",
    "            if sample['sample_id'] == fixed_samples[0]['sample_id']:\n",
    "                grid_top = np.hstack([\n",
    "                    normalize_for_visualization(seismic_slice),  # Y\n",
    "                    normalize_for_visualization(gt_slice)        # X\n",
    "                ])\n",
    "                grid_bottom = np.hstack([\n",
    "                    normalize_for_visualization(pred_slice),     # X_pred\n",
    "                    residual_norm                                # Residual (already normalized to 0..1 around 0)\n",
    "                ])\n",
    "                grid_img = np.vstack([grid_top, grid_bottom])\n",
    "\n",
    "                # Stable key (no epoch in the key): W&B will store history by step\n",
    "                visualizations_log[\"Viz/diagnostics/central_slice_grid\"] = wandb.Image(\n",
    "                    grid_img,\n",
    "                    caption=\"Top: Y, X | Bottom: X_pred, Residual\"\n",
    "                )\n",
    "            \n",
    "            # Metrics\n",
    "            reconstruction_mse = float(np.mean((pred_slice - gt_slice) ** 2))\n",
    "            condition_consistency_mse = float(np.mean((y_pred_3d - sample['seismic']) ** 2))\n",
    "            total_reconstruction_mse += reconstruction_mse\n",
    "            total_condition_consistency_mse += condition_consistency_mse\n",
    "            ssim_score = ssim(gt_slice, pred_slice, data_range=1.0)\n",
    "            \n",
    "            # Well Logs\n",
    "            well_log_depth = np.arange(prediction_3d.shape[0]) * 25\n",
    "            well_x, well_y = prediction_3d.shape[1] // 2, prediction_3d.shape[2] // 2\n",
    "            gt_well_log = sample['karst'][:, well_x, well_y]\n",
    "            pred_well_log = prediction_3d[:, well_x, well_y]\n",
    "            log_correlation = np.corrcoef(gt_well_log, pred_well_log)[0, 1] if np.std(gt_well_log) > 0 and np.std(pred_well_log) > 0 else 0.0\n",
    "            \n",
    "            # Add data to the validation table\n",
    "            validation_table.add_data(\n",
    "            epoch,\n",
    "            sample['sample_id'],\n",
    "            wandb.Image(normalize_for_visualization(seismic_slice)),\n",
    "            wandb.Image(normalize_for_visualization(gt_slice)),\n",
    "            wandb.Image(normalize_for_visualization(pred_slice)),\n",
    "            wandb.Image(residual_norm, caption=\"Residual Map\"),\n",
    "            plot_well_log_comparison(gt_well_log, pred_well_log, well_log_depth),\n",
    "            reconstruction_mse,                 # X_pred vs X (central slice)\n",
    "            condition_consistency_mse,          # Y_pred vs Y (full 3D)\n",
    "            ssim_score,\n",
    "            log_correlation\n",
    "            )\n",
    "\n",
    "        # Log validation metrics ONCE per epoch\n",
    "        # Log all epoch-level data in a single call\n",
    "        avg_recon = total_reconstruction_mse / len(fixed_samples)\n",
    "        avg_cond  = total_condition_consistency_mse / len(fixed_samples)\n",
    "\n",
    "        # Validation loss tied to actual validation metrics\n",
    "        val_total_loss = 0.45 * avg_recon + 0.55 * avg_cond\n",
    "        epoch_log_data = {\"val/total_loss\": val_total_loss, \"val/avg_reconstruction_mse\": total_reconstruction_mse / len(fixed_samples),\n",
    "        \"val/avg_condition_consistency_mse\": total_condition_consistency_mse / len(fixed_samples), \"val_table/validation_table\": validation_table}\n",
    "        epoch_log_data.update(visualizations_log) # Add the dictionary of 3D views\n",
    "        run.log(epoch_log_data)\n",
    "\n",
    "        # ===================================================================\n",
    "        # 5. W&B MODEL REGISTRY: Versioning Models as Artifacts\n",
    "        #    - We check if the model has improved and, if so, save it.\n",
    "        #    - wandb.Artifact() creates a versioned package of model files and metadata.\n",
    "        #    - run.link_artifact() registers the model in the W&B Model Registry,\n",
    "        #      assigning it an alias like \"best\" or \"staging\" for easy retrieval.\n",
    "        # ===================================================================\n",
    "        if val_total_loss < best_validation_loss:\n",
    "            best_validation_loss = val_total_loss\n",
    "            print(f\"New best model at epoch {epoch}! Validation loss: {val_total_loss:.4f}\")\n",
    "            \n",
    "            # Build a concise model card as markdown; this will render as the artifact/model “card”\n",
    "            model_card_md = dedent(f\"\"\"\\\n",
    "            # Model Card: Conditional Diffusion (Simulated)\n",
    "\n",
    "            ## Overview\n",
    "            - Task: Geological structure generation (conditional diffusion)\n",
    "            - Notation: X (GT karst), X_pred (generated), Y (seismic), Y_pred = f(X_pred), Residual = Y_pred − Y\n",
    "            - Demo visuals: single-channel; 3D at 0/mid/last; 2D grid every epoch\n",
    "\n",
    "            ## Data & Lineage\n",
    "            - Dataset artifact: `{artifact.name}`\n",
    "            - Validation samples: {len(fixed_samples)} (pre-validated)\n",
    "\n",
    "            ## Training Summary (this version)\n",
    "            - Epoch: {epoch}\n",
    "            - val/total_loss: {val_total_loss:.6f}\n",
    "            - val/avg_reconstruction_mse: {avg_recon:.6f}\n",
    "            - val/avg_condition_consistency_mse: {avg_cond:.6f}\n",
    "\n",
    "            ## Key Config\n",
    "            - epochs={config.get('epochs')}, batches_per_epoch={config.get('batches_per_epoch')}\n",
    "            - learning_rate={config.get('learning_rate')}, batch_size={config.get('batch_size')}\n",
    "            - noise_schedule={config.get('noise_schedule')}, conditioning_strength={config.get('conditioning_strength')}\n",
    "            - seed={config.get('seed')}\n",
    "\n",
    "            ## Limitations\n",
    "            - Simulated training and losses for reproducible demo\n",
    "            - Embedded 3D is downsampled for responsiveness\n",
    "            \"\"\").strip()\n",
    "            \n",
    "            checkpoint_artifact = wandb.Artifact(\n",
    "                name=\"conditional-diffusion-checkpoint\",\n",
    "                type=\"model\",\n",
    "                description=model_card_md,  # <- model card as markdown\n",
    "                metadata={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"validation_loss\": round(val_total_loss, 4),\n",
    "                    \"dataset_artifact\": artifact.name\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            checkpoint_path = f\"best_model_epoch_{epoch}.pth\"\n",
    "            Path(checkpoint_path).write_text(f\"Best model checkpoint at epoch {epoch}\")\n",
    "            checkpoint_artifact.add_file(checkpoint_path)\n",
    "            \n",
    "            # Log the artifact and link it to the registry\n",
    "            logged_artifact = run.log_artifact(checkpoint_artifact, aliases=[\"best\"])\n",
    "           \n",
    "            # run.link_artifact() registers the model in the W&B Model Registry. Aliases\n",
    "            # like \"staging\" or \"production\" create pointers for CI/CD systems, automating\n",
    "            # the path from training to deployment.\n",
    "            run.link_artifact(\n",
    "                artifact=logged_artifact,\n",
    "                target_path=\"wandb-registry-model/conditional-diffusion\",\n",
    "                aliases=[\"staging\"]\n",
    "            )\n",
    "            os.remove(checkpoint_path)\n",
    "\n",
    "    # ===================================================================\n",
    "    # 6. W&B CLEANUP: Finalizing the Run\n",
    "    #    - run.finish() marks the run as complete and uploads any remaining data.\n",
    "    # ===================================================================\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5ad2f",
   "metadata": {},
   "source": [
    "## 4. Execute a Single Training Run\n",
    "\n",
    "Now we call our main function to execute a single, baseline training run.\n",
    "\n",
    "**Action:** When you run this cell, click the W&B link that appears in the output. This will take you to the live dashboard where you can see all the metrics, images, and tables being logged in real-time. This is the central hub for our experiment.\n",
    "\n",
    "What to look for in the UI:\n",
    "\n",
    "Live Metrics: Watch the loss curves update in real-time. No need to wait for the job to finish.\n",
    "\n",
    "System Monitoring: Check the system tab to see live CPU/GPU utilization charts, automatically captured by W&B.\n",
    "\n",
    "Interactive Tables: At the end of each epoch, a new row will appear in the advanced_validation_table. Click on the images to expand them and hover over the charts to interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ddae711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_143300-wo5v3m9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad4/runs/wo5v3m9u' target=\"_blank\">wild-bush-4</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad4' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad4/runs/wo5v3m9u' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad4/runs/wo5v3m9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3642\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.3017\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2596\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n",
      "New best model at epoch 5! Validation loss: 1.2595\n",
      "Epoch 6: Simulating conditional diffusion training...\n",
      "New best model at epoch 6! Validation loss: 1.2571\n",
      "Epoch 7: Simulating conditional diffusion training...\n",
      "Epoch 8: Simulating conditional diffusion training...\n",
      "Epoch 9: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>██████████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▃▃▃▃▃▃▃▃▃▃▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>██████████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁</td></tr><tr><td>train/reconstruction_mse</td><td>██████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/total_loss</td><td>██████▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>▇▅▃▁▂▂▂▇██</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▇▇▇▆▅▅▁▁▁</td></tr><tr><td>val/total_loss</td><td>█▅▃▁▁▁▁▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.00086</td></tr><tr><td>train/noise_prediction_loss</td><td>0.48049</td></tr><tr><td>train/reconstruction_mse</td><td>0.13947</td></tr><tr><td>train/total_loss</td><td>0.34408</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.50051</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.00048</td></tr><tr><td>val/total_loss</td><td>1.37549</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-bush-4</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad4/runs/wo5v3m9u' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad4/runs/wo5v3m9u</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad4' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad4</a><br>Synced 8 W&B file(s), 26 media file(s), 119 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_143300-wo5v3m9u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train with default config\n",
    "# Use the example config we defined earlier \n",
    "train_conditional_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12db70",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Sweeps: Enterprise-Scale Optimization\n",
    "\n",
    "Running one experiment manually is not efficient. To find the optimal model, we need to explore the hyperparameter space. W&B Sweeps provide a powerful, scalable, and fully integrated way to automate this process.\n",
    "\n",
    "Instead of writing custom loops or relying on external optimization libraries, you can define a search strategy in a simple configuration file. W&B then coordinates the search, distributing jobs to any number of agents and providing powerful visualizations to track the results in real-time.\n",
    "\n",
    "First, we define a `sweep_config`. This configuration specifies the search strategy (Bayesian), the metric to optimize (validation loss), and the range of hyperparameters to test. We also include an early termination strategy to save compute resources by stopping underperforming runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b17e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rxh40hb8\n",
      "Sweep URL: https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8\n",
      "✅ Sweep created successfully! Sweep ID: rxh40hb8\n",
      "🧹 View and manage your sweep here: https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8\n"
     ]
    }
   ],
   "source": [
    "# Early termination: realistic Hyperband settings for epoch-based training\n",
    "# - Iteration == epoch (we log val/total_loss once per epoch)\n",
    "# - min_iter: let every run complete at least the first 3 epochs before pruning\n",
    "# - max_iter: equal to total epochs per run (6 here). If you raise epochs, raise this too\n",
    "# - eta: 3 means keep roughly the top 1/3 at each bracket; 2 is gentler, 4 is more aggressive\n",
    "# - Effect: poor configs get stopped around mid-training; strong configs run to completion\n",
    "early_stop = {\"type\": \"hyperband\", \"min_iter\": 3, \"max_iter\": 6, \"eta\": 3}\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val/total_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"distribution\": \"log_uniform_values\", \"min\": 1e-4, \"max\": 3e-3},\n",
    "        \"batch_size\": {\"values\": [4, 8]},\n",
    "        \"noise_schedule\": {\"values\": [\"cosine\", \"linear\", \"sigmoid\"]},\n",
    "        \"conditioning_strength\": {\"distribution\": \"uniform\", \"min\": 0.6, \"max\": 0.95},\n",
    "\n",
    "        # Short, deterministic runs for a demo\n",
    "        \"epochs\": {\"value\": 6},\n",
    "        \"batches_per_epoch\": {\"value\": 12},\n",
    "        \"seed\": {\"value\": 42},\n",
    "\n",
    "        # Keep sweeps light (no 3D during sweeps)\n",
    "        \"enable_3d\": {\"value\": False},\n",
    "        \"enable_high_fidelity_3d\": {\"value\": False},\n",
    "        \"enable_ipyvolume\": {\"value\": False},\n",
    "    },\n",
    "    \"early_terminate\": early_stop,\n",
    "}\n",
    "# 2. Initialize the Sweep\n",
    "# This command creates the sweep controller in the W&B cloud. It acts as a\n",
    "# central coordinator that agents can query for new jobs.\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_config,\n",
    "    entity=ENTITY,\n",
    "    project=PROJECT\n",
    ")\n",
    "\n",
    "print(f\"✅ Sweep created successfully! Sweep ID: {sweep_id}\")\n",
    "print(f\"🧹 View and manage your sweep here: https://wandb.ai/{ENTITY}/{PROJECT}/sweeps/{sweep_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07322a0",
   "metadata": {},
   "source": [
    "## 6. Launch a W&B Agent\n",
    "\n",
    "Now that the sweep is initialized, we launch an agent. The agent is a simple, stateless worker that will:\n",
    "1.  Ask the W&B sweep server for a set of hyperparameters.\n",
    "2.  Run our `train_conditional_diffusion` function with those hyperparameters.\n",
    "3.  Repeat until the sweep is finished.\n",
    "\n",
    "This architecture is incredibly scalable. You can launch agents on your laptop, on a fleet of cloud VMs, or in a Kubernetes cluster, and they will all coordinate through the central sweep controller to work on the same optimization problem\n",
    "\n",
    "We will start one agent to run 5 experiments for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d44a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 00lvj4c4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches_per_epoch: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconditioning_strength: 0.7615948697515844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_high_fidelity_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_ipyvolume: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001051215348245934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_schedule: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mallanstevenson\u001b[0m (\u001b[33mwandb_emea\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'workshop-scratchpad3' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'wandb_emea' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_140154-00lvj4c4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/00lvj4c4' target=\"_blank\">confused-sweep-1</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/00lvj4c4' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/00lvj4c4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3650\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.3008\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2590\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>████████▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▅▅▄▅▄▄▄▃▄▃▃▃▂▃▂▂▁</td></tr><tr><td>train/reconstruction_mse</td><td>███████▇██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▅▄▄▄▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>train/total_loss</td><td>███████████▇▇▇▇▇▇▆▆▆▆▆▅▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>█▅▃▁▂▂</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▆▅▅▂▁</td></tr><tr><td>val/total_loss</td><td>█▅▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.001</td></tr><tr><td>train/noise_prediction_loss</td><td>1.3089</td></tr><tr><td>train/reconstruction_mse</td><td>1.0655</td></tr><tr><td>train/total_loss</td><td>1.21154</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.20051</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.11095</td></tr><tr><td>val/total_loss</td><td>1.26021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-1</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/00lvj4c4' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/00lvj4c4</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a><br>Synced 8 W&B file(s), 12 media file(s), 75 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_140154-00lvj4c4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p58dy0ll with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches_per_epoch: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconditioning_strength: 0.8744591461858962\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_high_fidelity_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_ipyvolume: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006107806772819459\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_schedule: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-12' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'workshop-scratchpad3' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'wandb_emea' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_140230-p58dy0ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p58dy0ll' target=\"_blank\">dandy-sweep-2</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p58dy0ll' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p58dy0ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3549\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.2908\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2511\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>████▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>train/reconstruction_mse</td><td>████▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/total_loss</td><td>████▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>█▅▃▁▂▂</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▆▆▆▂▁</td></tr><tr><td>val/total_loss</td><td>█▅▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.00058</td></tr><tr><td>train/noise_prediction_loss</td><td>0.8507</td></tr><tr><td>train/reconstruction_mse</td><td>0.62607</td></tr><tr><td>train/total_loss</td><td>0.76085</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.20098</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.10714</td></tr><tr><td>val/total_loss</td><td>1.25875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-2</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p58dy0ll' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p58dy0ll</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a><br>Synced 8 W&B file(s), 12 media file(s), 75 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_140230-p58dy0ll/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p2p865cg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches_per_epoch: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconditioning_strength: 0.7985139256172038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_high_fidelity_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_ipyvolume: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00016665592994640757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_schedule: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-19' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'workshop-scratchpad3' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'wandb_emea' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_140257-p2p865cg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p2p865cg' target=\"_blank\">earthy-sweep-3</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p2p865cg' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p2p865cg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3625\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.2977\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2561\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>████▇█▇▇▇▇▇▇▆▇▇▇▇▆▆▆▆▆▅▅▅▄▅▄▄▃▃▃▄▂▃▂▂▂▁▁</td></tr><tr><td>train/reconstruction_mse</td><td>█████▇▇█████▇▇▆▇▇▇▇▆▅▆▅▆▅▅▅▅▄▄▃▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/total_loss</td><td>██████▇█▇▇██▇▇▇▇▇▇▆▆▆▅▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>█▅▃▁▂▂</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▆▆▆▂▁</td></tr><tr><td>val/total_loss</td><td>█▅▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/noise_prediction_loss</td><td>1.31365</td></tr><tr><td>train/reconstruction_mse</td><td>1.06889</td></tr><tr><td>train/total_loss</td><td>1.21575</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.20064</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.10966</td></tr><tr><td>val/total_loss</td><td>1.2597</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-3</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p2p865cg' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/p2p865cg</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a><br>Synced 8 W&B file(s), 12 media file(s), 75 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_140257-p2p865cg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: px7bds8m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches_per_epoch: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconditioning_strength: 0.9247808425633042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_high_fidelity_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_ipyvolume: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0011944523867646495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_schedule: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-26' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'workshop-scratchpad3' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'wandb_emea' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_140333-px7bds8m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/px7bds8m' target=\"_blank\">brisk-sweep-4</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/px7bds8m' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/px7bds8m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3546\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.2882\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2475\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>████▇██▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▅▄▄▃▃▄▃▂▂▃▁▁</td></tr><tr><td>train/reconstruction_mse</td><td>█████▇▇▇██▇▇▇▇▆▇▆▆▆▆▅▅▅▅▅▄▅▄▄▄▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/total_loss</td><td>█████▇▇▇█▇▇▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▁▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>█▅▃▁▂▃</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▇▆▇▂▁</td></tr><tr><td>val/total_loss</td><td>█▅▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.00113</td></tr><tr><td>train/noise_prediction_loss</td><td>1.2901</td></tr><tr><td>train/reconstruction_mse</td><td>1.02714</td></tr><tr><td>train/total_loss</td><td>1.18492</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.20124</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.10557</td></tr><tr><td>val/total_loss</td><td>1.25819</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sweep-4</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/px7bds8m' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/px7bds8m</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a><br>Synced 8 W&B file(s), 12 media file(s), 75 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_140333-px7bds8m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 03dipu8e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatches_per_epoch: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconditioning_strength: 0.8998692112735487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_high_fidelity_3d: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenable_ipyvolume: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0010673011001681597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_schedule: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-33' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'workshop-scratchpad3' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'wandb_emea' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/al/Documents/gitstuff/shell-tenet-28July2025/wandb/run-20250809_140409-03dipu8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/03dipu8e' target=\"_blank\">northern-sweep-5</a></strong> to <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/sweeps/rxh40hb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/03dipu8e' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/03dipu8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   49 of 49 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 real geological samples from CigKarst\n",
      "Selected 3 pre-validated samples for tracking.\n",
      "Epoch 0: Simulating conditional diffusion training...\n",
      "New best model at epoch 0! Validation loss: 1.4434\n",
      "Epoch 1: Simulating conditional diffusion training...\n",
      "New best model at epoch 1! Validation loss: 1.3533\n",
      "Epoch 2: Simulating conditional diffusion training...\n",
      "New best model at epoch 2! Validation loss: 1.2890\n",
      "Epoch 3: Simulating conditional diffusion training...\n",
      "New best model at epoch 3! Validation loss: 1.2495\n",
      "Epoch 4: Simulating conditional diffusion training...\n",
      "Epoch 5: Simulating conditional diffusion training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/noise_prediction_loss</td><td>███▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/reconstruction_mse</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_loss</td><td>████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>█▅▃▁▂▂</td></tr><tr><td>val/avg_reconstruction_mse</td><td>█▆▆▆▂▁</td></tr><tr><td>val/total_loss</td><td>█▅▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/learning_rate</td><td>0.00101</td></tr><tr><td>train/noise_prediction_loss</td><td>0.83672</td></tr><tr><td>train/reconstruction_mse</td><td>0.61658</td></tr><tr><td>train/total_loss</td><td>0.74867</td></tr><tr><td>val/avg_condition_consistency_mse</td><td>2.20111</td></tr><tr><td>val/avg_reconstruction_mse</td><td>0.10634</td></tr><tr><td>val/total_loss</td><td>1.25846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-5</strong> at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/03dipu8e' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3/runs/03dipu8e</a><br> View project at: <a href='https://wandb.ai/wandb_emea/workshop-scratchpad3' target=\"_blank\">https://wandb.ai/wandb_emea/workshop-scratchpad3</a><br>Synced 8 W&B file(s), 12 media file(s), 75 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250809_140409-03dipu8e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Run the Sweep Agent\n",
    "# This single command connects a worker to the sweep controller. The agent\n",
    "# automatically fetches a configuration, executes the training function,\n",
    "# and reports the results back, requiring zero manual orchestration.\n",
    "wandb.agent(sweep_id, function=train_conditional_diffusion, count=5)\n",
    "wandb.teardown() # if we want to do normal runs after a sweep, in the same session, we must run this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b8187",
   "metadata": {},
   "source": [
    "## 7. Programmatic Executive Reports: From Model to Boardroom\n",
    "\n",
    "The final step is to bridge the gap between technical results and business stakeholders. This is often the \"last mile\" problem in ML, where valuable insights get lost in translation. Manually creating reports is slow, error-prone, and creates static documents that are quickly outdated.\n",
    "\n",
    "The W&B Report API allows us to programmatically generate dynamic, data-driven reports directly from our experiments. This transforms our experimental results from raw data into a persistent, shareable decision-making asset. This provides a transparent, repeatable, and always up-to-date summary for governance and high-stakes investment decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06cc5105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-40' coro=<Event.wait() running at /opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/locks.py:214> wait_for=<Future cancelled>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a programmatic report...\n",
      "Using entity: 'wandb_emea', project: 'workshop-scratchpad3'\n",
      "Generating report based on best run: neat-eon-9 (ID: uv0hadse)\n",
      "Best validation loss: 0.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Saved report to: https://wandb.ai/wandb_emea/workshop-scratchpad3/reports/Geological-ML-Model-Performance---2025-08-09--VmlldzoxMzkyMzUwOQ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Executive report created successfully!\n",
      "📊 Title: Geological ML Model Performance - 2025-08-09\n",
      "🔗 URL: https://wandb.ai/wandb_emea/workshop-scratchpad3/reports/Geological-ML-Model-Performance---2025-08-09--VmlldzoxMzkyMzUwOQ==\n",
      "📧 Ready for stakeholder distribution.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating a programmatic report...\")\n",
    "print(f\"Using entity: '{ENTITY}', project: '{PROJECT}'\")\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import wandb_workspaces.reports.v2 as wr\n",
    "\n",
    "try:\n",
    "    # 1) Find the best finished run by val/total_loss\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(\n",
    "        path=f\"{ENTITY}/{PROJECT}\",\n",
    "        order=\"-created_at\",\n",
    "        filters={\"tags\": \"conditional-diffusion\", \"state\": \"finished\"},\n",
    "    )\n",
    "    if not runs:\n",
    "        raise ValueError(\"No finished runs with tag 'conditional-diffusion' found.\")\n",
    "\n",
    "    best_run = sorted(\n",
    "        runs, key=lambda r: r.summary.get(\"val/total_loss\", float(\"inf\"))\n",
    "    )[0]\n",
    "    best_val = best_run.summary.get(\"val/total_loss\", None)\n",
    "    best_val_str = f\"{best_val:.4f}\" if best_val is not None else \"N/A\"\n",
    "    print(f\"Generating report based on best run: {best_run.name} (ID: {best_run.id})\")\n",
    "    if best_val is not None:\n",
    "        print(f\"Best validation loss: {best_val_str}\")\n",
    "\n",
    "    # 2) Build the report container (full-width page)\n",
    "    report = wr.Report(\n",
    "        entity=ENTITY,\n",
    "        project=PROJECT,\n",
    "        title=f\"Geological ML Model Performance - {time.strftime('%Y-%m-%d')}\",\n",
    "        description=f\"Automated summary for the conditional diffusion model. Best run: {best_run.name}.\",\n",
    "        width=\"fluid\",  # full-width page container\n",
    "    )\n",
    "\n",
    "    # Target the best run only\n",
    "    runset = wr.Runset(entity=ENTITY, project=PROJECT, name=best_run.id)\n",
    "\n",
    "    # 3) Blocks\n",
    "    report.blocks = [\n",
    "        wr.H1(\"Executive Summary: Geological Interpretation Model\"),\n",
    "        wr.P(\n",
    "            f\"The model was trained for **{best_run.config.get('epochs', 'N/A')} epochs**, \"\n",
    "            f\"achieving a final validation loss of **{best_val_str}**. \"\n",
    "            f\"This automated report was generated on {time.strftime('%B %d, %Y')}.\"\n",
    "        ),\n",
    "\n",
    "        wr.H2(\"Key Performance Metrics\"),\n",
    "        wr.P(\"Validation loss over time (uses Step on the X axis).\"),\n",
    "\n",
    "        # Full-width line plot (defaults X to Step)\n",
    "        wr.PanelGrid(\n",
    "            runsets=[runset],\n",
    "            panels=[\n",
    "                wr.LinePlot(\n",
    "                    title=\"Validation Loss Over Training\",\n",
    "                    y=[\"val/total_loss\"],\n",
    "                    layout={\"w\": 24, \"h\": 12},  # set width on the panel\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        # Small summary scalar (optional, separate row)\n",
    "        wr.PanelGrid(\n",
    "            runsets=[runset],\n",
    "            panels=[\n",
    "                wr.ScalarChart(\n",
    "                    title=\"Best Final Validation Loss\",\n",
    "                    metric=\"val/total_loss\",\n",
    "                    layout={\"w\": 8, \"h\": 8},\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        wr.H2(\"Detailed Validation Analysis\"),\n",
    "        wr.P(\"Interactive table logged each epoch; single-channel slices and well-log comparison.\"),\n",
    "\n",
    "        # Full-width validation table (only panel in its grid)\n",
    "        wr.PanelGrid(\n",
    "            runsets=[runset],\n",
    "            panels=[\n",
    "                wr.WeavePanelSummaryTable(\n",
    "                    table_name=\"advanced_validation_table\",\n",
    "                    layout={\"w\": 24, \"h\": 20},  # full width on the panel\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        wr.H2(\"Model Governance & Next Steps\"),\n",
    "        wr.P(\n",
    "            f\"\"\"The best model artifact from this run has been versioned and linked to the Model Registry.\n",
    "\n",
    "- Best Model Run Link: [`{best_run.name}`](https://wandb.ai/{ENTITY}/{PROJECT}/runs/{best_run.id})\n",
    "- Dataset artifact version: `CigKarst:v0`\n",
    "- Model artifact name: `conditional-diffusion-checkpoint`\n",
    "- Current registry alias: `staging`\n",
    "\n",
    "Next Steps:\n",
    "1. Review model performance with geological subject matter experts.\n",
    "2. Promote the model from 'Staging' to 'Production' if acceptance criteria are met.\n",
    "3. Plan deployment to a validation environment.\n",
    "\"\"\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    report.save()\n",
    "    print(\"\\n✅ Executive report created successfully!\")\n",
    "    print(f\"📊 Title: {report.title}\")\n",
    "    print(f\"🔗 URL: {report.url}\")\n",
    "    print(\"📧 Ready for stakeholder distribution.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Report creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93963204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16aa64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
